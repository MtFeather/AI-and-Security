{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "我的第一個強化學習.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MtFeather/AI-and-Security/blob/master/tensorflow/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E5%80%8B%E5%BC%B7%E5%8C%96%E5%AD%B8%E7%BF%92.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vK853CM5RMmv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# cartpole_notes.py"
      ]
    },
    {
      "metadata": {
        "id": "l-Ctg4ljRP_W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Classic cart-pole system implemented by Rich Sutton et al.\n",
        "Copied from https://webdocs.cs.ualberta.ca/~sutton/book/code/pole.c\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import math\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CartPoleEnv(gym.Env):\n",
        "    metadata = {\n",
        "        'render.modes': ['human', 'rgb_array'],\n",
        "        'video.frames_per_second' : 50\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        self.gravity = 9.8                                   #重力加速度\n",
        "        self.masscart = 1.0                                  #小车的质量\n",
        "        self.masspole = 0.1                                  #摆的质量\n",
        "        self.total_mass = (self.masspole + self.masscart)    #总质量\n",
        "        self.length = 0.5 # actually half the pole's length  #摆的半长，即摆的转动中心到摆的质心的距离\n",
        "        self.polemass_length = (self.masspole * self.length) #摆的质量长\n",
        "        self.force_mag = 10.0                                #外力的振幅\n",
        "        self.tau = 0.02  # seconds between state updates     #更新时间步\n",
        "\n",
        "        # Angle at which to fail the episode\n",
        "        self.theta_threshold_radians = 12 * 2 * math.pi / 360 #摆角最大范围\n",
        "        self.x_threshold = 2.4                                #小车x方向最大运动范围\n",
        "\n",
        "        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n",
        "        high = np.array([\n",
        "            self.x_threshold * 2,\n",
        "            np.finfo(np.float32).max,\n",
        "            self.theta_threshold_radians * 2,\n",
        "            np.finfo(np.float32).max])\n",
        "\n",
        "        self.action_space = spaces.Discrete(2)                      #小车倒立摆的动作空间为2维离散空间\n",
        "        self.observation_space = spaces.Box(-high, high)            #观测空间为（x,dx,theta,dtheta）区间为（（-24，24），（-2.4，2.4））\n",
        "\n",
        "        self._seed()\n",
        "        self.viewer = None\n",
        "        self.state = None\n",
        "\n",
        "        self.steps_beyond_done = None\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def _step(self, action):\n",
        "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
        "        state = self.state\n",
        "        x, x_dot, theta, theta_dot = state\n",
        "        force = self.force_mag if action==1 else -self.force_mag\n",
        "        costheta = math.cos(theta)\n",
        "        sintheta = math.sin(theta)\n",
        "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
        "        thetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
        "        xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
        "        x  = x + self.tau * x_dot\n",
        "        x_dot = x_dot + self.tau * xacc\n",
        "        theta = theta + self.tau * theta_dot\n",
        "        theta_dot = theta_dot + self.tau * thetaacc\n",
        "        self.state = (x,x_dot,theta,theta_dot)\n",
        "        done =  x < -self.x_threshold \\\n",
        "                or x > self.x_threshold \\\n",
        "                or theta < -self.theta_threshold_radians \\\n",
        "                or theta > self.theta_threshold_radians\n",
        "        done = bool(done)\n",
        "\n",
        "        if not done:\n",
        "            reward = 1.0\n",
        "        elif self.steps_beyond_done is None:\n",
        "            # Pole just fell!\n",
        "            self.steps_beyond_done = 0\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            if self.steps_beyond_done == 0:\n",
        "                logger.warning(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
        "            self.steps_beyond_done += 1\n",
        "            reward = 0.0\n",
        "\n",
        "        return np.array(self.state), reward, done, {}\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
        "        self.steps_beyond_done = None\n",
        "        return np.array(self.state)\n",
        "\n",
        "    def _render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if self.viewer is not None:\n",
        "                self.viewer.close()\n",
        "                self.viewer = None\n",
        "            return\n",
        "\n",
        "        screen_width = 600\n",
        "        screen_height = 400\n",
        "\n",
        "        world_width = self.x_threshold*2\n",
        "        scale = screen_width/world_width\n",
        "        carty = 100 # TOP OF CART\n",
        "        polewidth = 10.0\n",
        "        polelen = scale * 1.0\n",
        "        cartwidth = 50.0\n",
        "        cartheight = 30.0\n",
        "\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
        "            # 创建台车\n",
        "            l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
        "            axleoffset =cartheight/4.0\n",
        "            cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
        "            #添加台车转换矩阵属性\n",
        "            self.carttrans = rendering.Transform()\n",
        "            cart.add_attr(self.carttrans)\n",
        "            #加入几何体台车\n",
        "            self.viewer.add_geom(cart)\n",
        "            #创建摆杆\n",
        "            l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
        "            pole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
        "            pole.set_color(.8,.6,.4)\n",
        "            #添加摆杆转换矩阵属性\n",
        "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
        "            pole.add_attr(self.poletrans)\n",
        "            pole.add_attr(self.carttrans)\n",
        "            #加入几何体\n",
        "            self.viewer.add_geom(pole)\n",
        "            #创建摆杆和台车之间的连接\n",
        "            self.axle = rendering.make_circle(polewidth/2)\n",
        "            self.axle.add_attr(self.poletrans)\n",
        "            self.axle.add_attr(self.carttrans)\n",
        "            self.axle.set_color(.5,.5,.8)\n",
        "            self.viewer.add_geom(self.axle)\n",
        "            #创建台车来回滑动的轨道，即一条直线\n",
        "            self.track = rendering.Line((0,carty), (screen_width,carty))\n",
        "            self.track.set_color(0,0,0)\n",
        "            self.viewer.add_geom(self.track)\n",
        "\n",
        "        if self.state is None: return None\n",
        "\n",
        "        x = self.state\n",
        "        cartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
        "        #设置平移属性\n",
        "        self.carttrans.set_translation(cartx, carty)\n",
        "        self.poletrans.set_rotation(-x[2])\n",
        "\n",
        "        return self.viewer.render(return_rgb_array = mode=='rgb_array')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tIsTkY8KR1g9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# grid_mdp.py"
      ]
    },
    {
      "metadata": {
        "id": "3vAcHX2BRUBm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy\n",
        "import random\n",
        "from gym import spaces\n",
        "import gym\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GridEnv(gym.Env):\n",
        "    metadata = {\n",
        "        'render.modes': ['human', 'rgb_array'],\n",
        "        'video.frames_per_second': 2\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.states = [1,2,3,4,5,6,7,8] #状态空间\n",
        "        self.x=[140,220,300,380,460,140,300,460]\n",
        "        self.y=[250,250,250,250,250,150,150,150]\n",
        "        self.terminate_states = dict()  #终止状态为字典格式\n",
        "        self.terminate_states[6] = 1\n",
        "        self.terminate_states[7] = 1\n",
        "        self.terminate_states[8] = 1\n",
        "\n",
        "        self.actions = ['n','e','s','w']\n",
        "\n",
        "        self.rewards = dict();        #回报的数据结构为字典\n",
        "        self.rewards['1_s'] = -1.0\n",
        "        self.rewards['3_s'] = 1.0\n",
        "        self.rewards['5_s'] = -1.0\n",
        "\n",
        "        self.t = dict();             #状态转移的数据格式为字典\n",
        "        self.t['1_s'] = 6\n",
        "        self.t['1_e'] = 2\n",
        "        self.t['2_w'] = 1\n",
        "        self.t['2_e'] = 3\n",
        "        self.t['3_s'] = 7\n",
        "        self.t['3_w'] = 2\n",
        "        self.t['3_e'] = 4\n",
        "        self.t['4_w'] = 3\n",
        "        self.t['4_e'] = 5\n",
        "        self.t['5_s'] = 8\n",
        "        self.t['5_w'] = 4\n",
        "\n",
        "        self.gamma = 0.8         #折扣因子\n",
        "        self.viewer = None\n",
        "        self.state = None\n",
        "\n",
        "    def getTerminal(self):\n",
        "        return self.terminate_states\n",
        "\n",
        "    def getGamma(self):\n",
        "        return self.gamma\n",
        "\n",
        "    def getStates(self):\n",
        "        return self.states\n",
        "\n",
        "    def getAction(self):\n",
        "        return self.actions\n",
        "    def getTerminate_states(self):\n",
        "        return self.terminate_states\n",
        "    def setAction(self,s):\n",
        "        self.state=s\n",
        "\n",
        "    def _step(self, action):\n",
        "        #系统当前状态\n",
        "        state = self.state\n",
        "        if state in self.terminate_states:\n",
        "            return state, 0, True, {}\n",
        "        key = \"%d_%s\"%(state, action)   #将状态和动作组成字典的键值\n",
        "\n",
        "        #状态转移\n",
        "        if key in self.t:\n",
        "            next_state = self.t[key]\n",
        "        else:\n",
        "            next_state = state\n",
        "        self.state = next_state\n",
        "\n",
        "        is_terminal = False\n",
        "\n",
        "        if next_state in self.terminate_states:\n",
        "            is_terminal = True\n",
        "\n",
        "        if key not in self.rewards:\n",
        "            r = 0.0\n",
        "        else:\n",
        "            r = self.rewards[key]\n",
        "\n",
        "\n",
        "        return next_state, r,is_terminal,{}\n",
        "    def _reset(self):\n",
        "        self.state = self.states[int(random.random() * len(self.states))]\n",
        "        return self.state\n",
        "    def render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            if self.viewer is not None:\n",
        "                self.viewer.close()\n",
        "                self.viewer = None\n",
        "            return\n",
        "        screen_width = 600\n",
        "        screen_height = 400\n",
        "\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
        "            #创建网格世界\n",
        "            self.line1 = rendering.Line((100,300),(500,300))\n",
        "            self.line2 = rendering.Line((100, 200), (500, 200))\n",
        "            self.line3 = rendering.Line((100, 300), (100, 100))\n",
        "            self.line4 = rendering.Line((180, 300), (180, 100))\n",
        "            self.line5 = rendering.Line((260, 300), (260, 100))\n",
        "            self.line6 = rendering.Line((340, 300), (340, 100))\n",
        "            self.line7 = rendering.Line((420, 300), (420, 100))\n",
        "            self.line8 = rendering.Line((500, 300), (500, 100))\n",
        "            self.line9 = rendering.Line((100, 100), (180, 100))\n",
        "            self.line10 = rendering.Line((260, 100), (340, 100))\n",
        "            self.line11 = rendering.Line((420, 100), (500, 100))\n",
        "            #创建第一个骷髅\n",
        "            self.kulo1 = rendering.make_circle(40)\n",
        "            self.circletrans = rendering.Transform(translation=(140,150))\n",
        "            self.kulo1.add_attr(self.circletrans)\n",
        "            self.kulo1.set_color(0,0,0)\n",
        "            #创建第二个骷髅\n",
        "            self.kulo2 = rendering.make_circle(40)\n",
        "            self.circletrans = rendering.Transform(translation=(460, 150))\n",
        "            self.kulo2.add_attr(self.circletrans)\n",
        "            self.kulo2.set_color(0, 0, 0)\n",
        "            #创建金条\n",
        "            self.gold = rendering.make_circle(40)\n",
        "            self.circletrans = rendering.Transform(translation=(300, 150))\n",
        "            self.gold.add_attr(self.circletrans)\n",
        "            self.gold.set_color(1, 0.9, 0)\n",
        "            #创建机器人\n",
        "            self.robot= rendering.make_circle(30)\n",
        "            self.robotrans = rendering.Transform()\n",
        "            self.robot.add_attr(self.robotrans)\n",
        "            self.robot.set_color(0.8, 0.6, 0.4)\n",
        "\n",
        "            self.line1.set_color(0, 0, 0)\n",
        "            self.line2.set_color(0, 0, 0)\n",
        "            self.line3.set_color(0, 0, 0)\n",
        "            self.line4.set_color(0, 0, 0)\n",
        "            self.line5.set_color(0, 0, 0)\n",
        "            self.line6.set_color(0, 0, 0)\n",
        "            self.line7.set_color(0, 0, 0)\n",
        "            self.line8.set_color(0, 0, 0)\n",
        "            self.line9.set_color(0, 0, 0)\n",
        "            self.line10.set_color(0, 0, 0)\n",
        "            self.line11.set_color(0, 0, 0)\n",
        "\n",
        "            self.viewer.add_geom(self.line1)\n",
        "            self.viewer.add_geom(self.line2)\n",
        "            self.viewer.add_geom(self.line3)\n",
        "            self.viewer.add_geom(self.line4)\n",
        "            self.viewer.add_geom(self.line5)\n",
        "            self.viewer.add_geom(self.line6)\n",
        "            self.viewer.add_geom(self.line7)\n",
        "            self.viewer.add_geom(self.line8)\n",
        "            self.viewer.add_geom(self.line9)\n",
        "            self.viewer.add_geom(self.line10)\n",
        "            self.viewer.add_geom(self.line11)\n",
        "            self.viewer.add_geom(self.kulo1)\n",
        "            self.viewer.add_geom(self.kulo2)\n",
        "            self.viewer.add_geom(self.gold)\n",
        "            self.viewer.add_geom(self.robot)\n",
        "\n",
        "        if self.state is None: return None\n",
        "        #self.robotrans.set_translation(self.x[self.state-1],self.y[self.state-1])\n",
        "        self.robotrans.set_translation(self.x[self.state-1], self.y[self.state- 1])\n",
        "\n",
        "\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UKDBPJ6TR9bm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# learning_and_test.py"
      ]
    },
    {
      "metadata": {
        "id": "KHOPkCCcR_5u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import gym\n",
        "from qlearning import *\n",
        "import time\n",
        "from gym import wrappers\n",
        "#main函数\n",
        "if __name__ == \"__main__\":\n",
        "   # grid = grid_mdp.Grid_Mdp()  # 创建网格世界\n",
        "    #states = grid.getStates()  # 获得网格世界的状态空间\n",
        "    #actions = grid.getAction()  # 获得网格世界的动作空间\n",
        "    sleeptime=0.5\n",
        "    terminate_states= grid.env.getTerminate_states()\n",
        "    #读入最优值函数\n",
        "    read_best()\n",
        "#    plt.figure(figsize=(12,6))\n",
        "    #训练\n",
        "    qfunc = dict()\n",
        "    qfunc = qlearning(num_iter1=500, alpha=0.2, epsilon=0.2)\n",
        "    #画图\n",
        "    plt.xlabel(\"number of iterations\")\n",
        "    plt.ylabel(\"square errors\")\n",
        "    plt.legend()\n",
        "   # 显示误差图像\n",
        "    plt.show()\n",
        "    time.sleep(sleeptime)\n",
        "    #学到的值函数\n",
        "    for s in states:\n",
        "        for a in actions:\n",
        "            key = \"%d_%s\"%(s,a)\n",
        "            print(\"the qfunc of key (%s) is %f\" %(key, qfunc[key]) )\n",
        "            qfunc[key]\n",
        "    #学到的策略为：\n",
        "    print(\"the learned policy is:\")\n",
        "    for i in range(len(states)):\n",
        "        if states[i] in terminate_states:\n",
        "            print(\"the state %d is terminate_states\"%(states[i]))\n",
        "        else:\n",
        "            print(\"the policy of state %d is (%s)\" % (states[i], greedy(qfunc, states[i])))\n",
        "    # 设置系统初始状态\n",
        "    s0 = 1\n",
        "    grid.env.setAction(s0)\n",
        "    # 对训练好的策略进行测试\n",
        "    grid = wrappers.Monitor(grid, './robotfindgold', force=True)  # 记录回放动画\n",
        "   #随机初始化，寻找金币的路径\n",
        "    for i in range(20):\n",
        "        #随机初始化\n",
        "        s0 = grid.reset()\n",
        "        grid.render()\n",
        "        time.sleep(sleeptime)\n",
        "        t = False\n",
        "        count = 0\n",
        "        #判断随机状态是否在终止状态中\n",
        "        if s0 in terminate_states:\n",
        "            print(\"reach the terminate state %d\" % (s0))\n",
        "        else:\n",
        "            while False == t and count < 100:\n",
        "                a1 = greedy(qfunc, s0)\n",
        "                print(s0, a1)\n",
        "                grid.render()\n",
        "                time.sleep(sleeptime)\n",
        "                key = \"%d_%s\" % (s0, a)\n",
        "                # 与环境进行一次交互，从环境中得到新的状态及回报\n",
        "                s1, r, t, i = grid.step(a1)\n",
        "                if True == t:\n",
        "                    #打印终止状态\n",
        "                    print(s1)\n",
        "                    grid.render()\n",
        "                    time.sleep(sleeptime)\n",
        "                    print(\"reach the terminate state %d\" % (s1))\n",
        "                # s1处的最大动作\n",
        "                s0 = s1\n",
        "                count += 1\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7X8c1JqPSF9X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# learning_cartpole.py"
      ]
    },
    {
      "metadata": {
        "id": "rmZPhsTsSMmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "3c248347-cbfd-4a6f-d4cb-f6123a7fd499"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from policynet import PolicyGradient\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "DISPLAY_REWARD_THRESHOLD = 1000\n",
        "RENDER = False\n",
        "\n",
        "#创建一个环境\n",
        "env = gym.make('CartPole-v0')\n",
        "env.seed(1)\n",
        "env = env.unwrapped\n",
        "\n",
        "print(env.action_space)\n",
        "print(env.observation_space)\n",
        "print(env.observation_space.high)\n",
        "print(env.observation_space.low)\n",
        "\n",
        "RL = PolicyGradient(\n",
        "    n_actions=env.action_space.n,\n",
        "    n_features=env.observation_space.shape[0],\n",
        "    learning_rate=0.02,\n",
        "    reward_decay=0.99,\n",
        "\n",
        ")\n",
        "#学习过程\n",
        "for i_episode in range(85):\n",
        "    observation = env.reset()\n",
        "    while True:\n",
        "        if RENDER: env.render()\n",
        "        #采样动作，探索环境\n",
        "        # action = RL.choose_action(observation)\n",
        "        # observation_, reward, done, info = env.step(action)\n",
        "        action = RL.choose_action(observation)\n",
        "\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "\n",
        "        #将观测，动作和回报存储起来\n",
        "        RL.store_transition(observation, action, reward)\n",
        "        if done:\n",
        "            ep_rs_sum = sum(RL.ep_rs)\n",
        "            if 'running_reward' not in globals():\n",
        "                running_reward = ep_rs_sum\n",
        "            else:\n",
        "                running_reward = running_reward * 0.99+ep_rs_sum * 0.01\n",
        "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True\n",
        "            print(\"episode:\", i_episode, \"rewards:\", int(running_reward))\n",
        "            #每个episode学习一次\n",
        "            vt = RL.learn()\n",
        "            if i_episode == 0:\n",
        "                plt.plot(vt)\n",
        "                plt.xlabel('episode steps')\n",
        "                plt.ylabel('normalized state-action value')\n",
        "                plt.show()\n",
        "            break\n",
        "\n",
        "        #智能体探索一步\n",
        "        observation = observation_\n",
        "# #测试过程\n",
        "for i in range(10):\n",
        "    observation = env.reset()\n",
        "    count = 0\n",
        "    while True:\n",
        "        # 采样动作，探索环境\n",
        "        env.render()\n",
        "        action = RL.greedy(observation)\n",
        "        #action = RL.choose_action(observation)\n",
        "        #action = RL.sample_action(observation)\n",
        "        # print (action)\n",
        "        # print(action1)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            print(count)\n",
        "            break\n",
        "        observation = observation_\n",
        "        count+=1\n",
        "        #time.sleep(0.001)\n",
        "        print (count)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b99cbc0fb42b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mRL_brain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPolicyGradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'RL_brain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}